{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from operator import itemgetter\n",
    "from scipy.spatial import distance\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from collections import defaultdict\n",
    "from itertools import product, combinations\n",
    "from operator import itemgetter\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# These are the functions you need to read in the data. You need to have the datafiles in a data folder that is in\n",
    "# the directory you are working in\n",
    "#LINK IM USING FOR K-NN https://stackabuse.com/k-nearest-neighbors-algorithm-in-python-and-scikit-learn/\n",
    "\n",
    "\n",
    "def parse(path):\n",
    "    g = gzip.open(path, 'rb')\n",
    "    for l in g:\n",
    "        yield eval(l)\n",
    "\n",
    "def getData(name, attributes):\n",
    "    i = 0\n",
    "    dic = {}\n",
    "    path = 'data/reviews_%s_5.json.gz' % name\n",
    "    for line in parse(path):\n",
    "        filtered = {}\n",
    "        for k in line.keys():\n",
    "            if k in attributes:\n",
    "                filtered[k] = line[k]\n",
    "            else:\n",
    "                continue\n",
    "        \n",
    "        dic[i] = filtered\n",
    "        i += 1\n",
    "    return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# These are one of the three names you can choose to read the dataset from\n",
    "names = ['Digital_Music', 'Kindle_Store', 'Video_Games']\n",
    "\n",
    "# These are all the possible attributes that our datapoints can have. Lots of them are not that useful for us. So during\n",
    "# the reading of the datafile you have to specify which of these attributes you want to include. For our purposes,\n",
    "# only reviewerID, asin (product ID) and overall will be helpful. But I included them all, just in case.\n",
    "attributes = ['reviewerID', 'asin', 'reviewerName', 'helpful', 'reviewText', 'overall', 'summary', \n",
    "              'unixReviewTime', 'reviewTime']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "run_control": {
     "marked": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# You read in the data for example like this:\n",
    "data = getData('Digital_Music', ['reviewerID', 'asin', 'overall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "run_control": {
     "marked": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reviewerID': 'A3EBHHCZO6V2A4', 'asin': '5555991584', 'overall': 5.0}\n",
      "{'reviewerID': 'AZPWAXJG9OJXV', 'asin': '5555991584', 'overall': 5.0}\n",
      "{'reviewerID': 'A38IRL0X2T4DPF', 'asin': '5555991584', 'overall': 5.0}\n",
      "{'reviewerID': 'A22IK3I6U76GX0', 'asin': '5555991584', 'overall': 5.0}\n",
      "{'reviewerID': 'A1AISPOIIHTHXX', 'asin': '5555991584', 'overall': 4.0}\n"
     ]
    }
   ],
   "source": [
    "# This is what our data will look like. You have a dictionary with integers as keys [0,1,2,3,4...], and as values one \n",
    "# datapoint. Each datapoint in itself is a dictionary, with as keys the attribute, and as value the value of that attribute.\n",
    "for i in range(5):\n",
    "    print(data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "def split_data(data, test_ratio=0.1):\n",
    "    \"\"\"This function splits the data into a train and test set, randomly extracting users and all their ratings to put\n",
    "    them in the test set. The test ratio can be specified as a number between 0 and 1, depending on how much percent of\n",
    "    the data should be in the test set.\"\"\"\n",
    "    reviewers = set()\n",
    "    for i in data.items():\n",
    "        reviewers.add(i[1]['reviewerID'])\n",
    "    \n",
    "    l = len(reviewers)\n",
    "    test_reviewers = set(random.sample(reviewers, int(l*test_ratio)))\n",
    "    train_reviewers = reviewers - test_reviewers\n",
    "    \n",
    "    train_data = {k:v for k,v in data.items() if v['reviewerID'] in train_reviewers}\n",
    "    test_data = {k:v for k,v in data.items() if v['reviewerID'] in test_reviewers}\n",
    "    \n",
    "    \n",
    "    tr = len(train_data)\n",
    "    te = len(test_data)\n",
    "    print('There are %s train reviews' %tr)\n",
    "    print('There are %s test reviews' %te)\n",
    "    \n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "run_control": {
     "marked": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 57236 train reviews\n",
      "There are 7470 test reviews\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = split_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "run_control": {
     "marked": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of colums in training Dataframe :  3\n",
      "Number of rows in training Dataframe :  57236\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>A3EBHHCZO6V2A4</td>\n",
       "      <td>5555991584</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>AZPWAXJG9OJXV</td>\n",
       "      <td>5555991584</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>A38IRL0X2T4DPF</td>\n",
       "      <td>5555991584</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>A22IK3I6U76GX0</td>\n",
       "      <td>5555991584</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>A1AISPOIIHTHXX</td>\n",
       "      <td>5555991584</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       reviewerID        asin  overall\n",
       "0  A3EBHHCZO6V2A4  5555991584      5.0\n",
       "1   AZPWAXJG9OJXV  5555991584      5.0\n",
       "2  A38IRL0X2T4DPF  5555991584      5.0\n",
       "3  A22IK3I6U76GX0  5555991584      5.0\n",
       "4  A1AISPOIIHTHXX  5555991584      4.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_train = pd.DataFrame.from_dict(train_data, orient='index')\n",
    "ds_test = pd.DataFrame.from_dict(test_data, orient='index')\n",
    "\n",
    "print('Number of colums in training Dataframe : ', len(ds_train.columns))\n",
    "print('Number of rows in training Dataframe : ', len(ds_train.index))\n",
    "ds_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['A3EBHHCZO6V2A4' '5555991584' 5.0]\n",
      " ['AZPWAXJG9OJXV' '5555991584' 5.0]\n",
      " ['A38IRL0X2T4DPF' '5555991584' 5.0]\n",
      " ['A22IK3I6U76GX0' '5555991584' 5.0]\n",
      " ['A1AISPOIIHTHXX' '5555991584' 4.0]\n",
      " ['A2P49WD75WHAG5' '5555991584' 5.0]]\n",
      "[['A24N1BAS3CU27H' '5555991584' 5.0]\n",
      " ['A200C7YQJ45LRR' 'B0000000ZW' 4.0]\n",
      " ['A8MWQY1R5YE3M' 'B00000016T' 5.0]\n",
      " ['A4VVYYB68NL4Z' 'B00000016T' 4.0]\n",
      " ['A22N03OBDDVSEB' 'B00000016T' 5.0]\n",
      " ['A3464G00K8ZYD1' 'B00000016T' 5.0]]\n"
     ]
    }
   ],
   "source": [
    "X_train = ds_train.iloc[:,:].values\n",
    "X_test = ds_test.iloc[:,:].values\n",
    "\n",
    "print(X_train[:6])\n",
    "print(X_test[:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# These are the three similarity measures that we make use of. Cosine looks at the angle between two datapoints, euclidean\n",
    "# at their distance in space, and pearson correlation is a measure of their linear dependence.\n",
    "\n",
    "def cosine_similarity(p,q):\n",
    "    d = sum(pi * qi for pi,qi in zip(p, q))\n",
    "    mag_p = math.sqrt(sum([pi**2 for pi in p]))\n",
    "    mag_q = math.sqrt(sum([qi**2 for qi in q]))\n",
    "    sim = d / ( mag_p * mag_q)\n",
    "    return sim\n",
    "\n",
    "def euclidean_similarity(p, q):\n",
    "    dist = math.sqrt(sum((pi-qi)**2 for pi,qi in zip(p, q)))\n",
    "    sim = 1 / (1+dist)\n",
    "    return sim    \n",
    "\n",
    "def pearson_correlation(p,q):\n",
    "    # this code does not scale well to large datasets. In the following, we rely on \n",
    "    # scipy.spatial.distance.correlation() to compute long vectors\n",
    "    if len(p) > 99:\n",
    "        return 1 - distance.correlation(p,q)        \n",
    "    \n",
    "    p_mean = sum(p) / len(p)\n",
    "    p_deviations = [(pi-p_mean) for pi in p]\n",
    "    \n",
    "    q_mean = sum(q) / len(q)\n",
    "    q_deviations = [(qi-q_mean) for qi in q]\n",
    "    \n",
    "    cov = sum(pd * qd for pd,qd in zip(p_deviations, q_deviations))\n",
    "        \n",
    "    sds_product = math.sqrt(sum((pd)**2 for pd in p_deviations) * sum((qd)**2 for qd in q_deviations))\n",
    "    \n",
    "    if sds_product != 0:\n",
    "        r = cov / sds_product\n",
    "    else:\n",
    "        r = 0\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "def calc_similarity(user2product, target, other_user, sim_measure, threshold=0):\n",
    "    \"\"\"This code extracts the set of shared rated items between two users, and computes their similarity. It is weighted\n",
    "    by the amount of shared items, divided by the total amount of items rated by the target user.\"\"\"\n",
    "    # found some explanation here https://towardsdatascience.com/introduction-to-recommender-systems-6c66cf15ada\n",
    "    \n",
    "    shared = list(set(user2product[target].keys()).intersection(set(user2product[other_user].keys())))\n",
    "    \n",
    "    if len(shared) <= threshold:\n",
    "        return 0\n",
    "    \n",
    "    target_ratings = [v for k,v in user2product[target].items() if k in shared]# for i in [target, other_user]]\n",
    "    other_user_ratings = [v for k,v in user2product[other_user].items() if k in shared]\n",
    "    \n",
    "    weight = len(shared)/len(user2product[target])\n",
    "    similarity = weight*sim_measure(target_ratings, other_user_ratings)\n",
    "    \n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "run_control": {
     "marked": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "user2item = defaultdict(dict)\n",
    "for reviewerID, asin, overall in X_train:\n",
    "    user2item[reviewerID][asin] = overall\n",
    "\n",
    "user2item_test = defaultdict(dict)\n",
    "for reviewerID, asin, overall in X_test:\n",
    "    user2item_test[reviewerID][asin] = overall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "run_control": {
     "marked": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no. of users: 4987\n",
      "no of reviews in user 1: 16\n"
     ]
    }
   ],
   "source": [
    "print(\"no. of users:\", len(user2item))\n",
    "print(\"no of reviews in user 1:\", len(list(user2item.items())[0][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no. of items: 3568\n",
      "no. of items: 2609\n"
     ]
    }
   ],
   "source": [
    "item2user = defaultdict(dict)\n",
    "for reviewerID, asin, overall in X_train:\n",
    "    item2user[asin][reviewerID] = overall\n",
    "    \n",
    "print(\"no. of items:\", len(item2user))\n",
    "\n",
    "item2user_test = defaultdict(dict)\n",
    "for reviewerID, asin, overall in X_test:\n",
    "    item2user_test[asin][reviewerID] = overall\n",
    "\n",
    "print(\"no. of items:\", len(item2user_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "#This function ranks the items within a dictionary of items and their ratings by users.\n",
    "def rankids(useritemdict, thresh):\n",
    "    rankedt = []\n",
    "    rankedb = []\n",
    "    scoredict = {}\n",
    "    \n",
    "    for k, v in useritemdict.items():\n",
    "        itemdict = v\n",
    "        for itemID, stars in itemdict.items():\n",
    "            p = (stars, k[1])\n",
    "            if itemID in scoredict:\n",
    "                scoredict[itemID].append(p)\n",
    "            else:\n",
    "                scoredict[itemID] = [p]\n",
    "    for ID, IDscortup in scoredict.items():\n",
    "        starlist = []\n",
    "        for scortup in IDscortup:\n",
    "            starlist.append(scortup[0])\n",
    "        a = np.mean(starlist)\n",
    "        q = []\n",
    "        for r in IDscortup:\n",
    "            q.append(r[0] * r[1])\n",
    "        IDandweighted = (ID, np.mean(q))\n",
    "        if a > thresh:\n",
    "            rankedt.append(IDandweighted)\n",
    "        else:\n",
    "            rankedb.append(IDandweighted)            \n",
    "    \n",
    "    rankedt.sort(key=itemgetter(1))\n",
    "    rankedb.sort(key=itemgetter(1))\n",
    "    rankedt.reverse()\n",
    "    rankedb.reverse()\n",
    "    return(rankedt, rankedb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "def recommend(userid, sim_measure, user2item, k, n):\n",
    "    \"\"\"This function takes a user id, similarity measure, the user2product dictionary, and k as input. It calculates the\n",
    "    k most similar users. It outputs a dictionary, with as keys tuples containing the k similar users ID and their\n",
    "    similarity, and as values a dictionary containing the ratings for all of their items\"\"\"\n",
    "    reviewers = [user for user in user2item.keys() if user != userid]\n",
    "    similarities = [(other_user, calc_similarity(user2item, userid, other_user, sim_measure)) for other_user in reviewers]\n",
    "    k_similarities = sorted(similarities, key = lambda x: x[1], reverse=True)[:k]\n",
    "    output = dict()\n",
    "    for user_tup in k_similarities:\n",
    "        output[user_tup] = user2item[user_tup[0]]\n",
    "    \n",
    "    lists = rankids(output, 2.49)\n",
    "    #return(output)\n",
    "    toplist = lists[0]\n",
    "    botlist = lists[1]\n",
    "    return (toplist + botlist)[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "#recommend('AZPWAXJG9OJXV', cosine_similarity, user2item, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "class kNN(object):\n",
    "    \"\"\" k-Nearest Neighbour \"\"\"\n",
    "    \n",
    "    def __init__(self, x2y: dict, sim_measure, name=None, k = 100, n=10, quiet = True):\n",
    "        print(\"Building object...\")\n",
    "        self.x2y = x2y                     # For example, user2item matrix\n",
    "        self.k = k\n",
    "        self.n = n\n",
    "        self.quiet = quiet\n",
    "        self.sim_measure = sim_measure     # Like cosine similarity\n",
    "        # The naming makes sure that you can load from a file\n",
    "        # Assuming the sample set is not changed\n",
    "        if name:\n",
    "            self.similarities=pickle.load(open(f\"data/{name}_similarities.pkl\", 'rb'))\n",
    "            self.neighborhood=pickle.load(open(f\"data/{name}_neighboorhood.pkl\", 'rb'))\n",
    "        else:\n",
    "            self.find_all_similarities()   # If the similarities are already stored\n",
    "            self.get_neighborhoods()   # Likewise\n",
    "        print(\"Done.\")\n",
    "    \n",
    "    def find_all_similarities(self):\n",
    "        \"\"\" Creates the similarity pairs for each item.\n",
    "        If the similarities are already loaded, calculations can be skipped. \"\"\"\n",
    "        if not self.quiet: print(\"Making a similarity matrix...\")\n",
    "        sims = defaultdict(dict)\n",
    "        for id1, id2 in combinations(self.x2y.keys(), 2):\n",
    "            sims[id1][id2] = calc_similarity(self.x2y, id1, id2, self.sim_measure)\n",
    "        self.similarities = sims\n",
    "        \n",
    "    def update_similarities(self, x2y_new):\n",
    "        \"\"\" Rather than recalculating similarities, this function takes in the test\n",
    "        set and adds the item similarities in that dictionary. \"\"\"\n",
    "        if not self.quiet: print(\"Updating...\")\n",
    "        for id1, id2 in product(x2y_new, self.x2y.keys()):\n",
    "            self.similarities[id1][id2] = calc_similarity(self.x2y, id1, id2, self.sim_measure)\n",
    "    \n",
    "    def get_neighborhoods(self):\n",
    "        \"\"\" For every item, it creates an item neighbourhood with similarity rates. \n",
    "        This is done via a default dictionary with values as dictionaries. \"\"\"\n",
    "        if not self.quiet: print(f\"Setting up the size-{self.k} neighbourhoods...\")\n",
    "        self.neighborhood = dict()\n",
    "        for x in self.similarities.keys():\n",
    "            self.neighborhood[x] = dict(sorted(self.similarities[x].items(),\n",
    "                                               key = itemgetter(1),\n",
    "                                               reverse = True)[:self.k])\n",
    "    \n",
    "    def getPredictionsForItems(self, an_id, predict_item):\n",
    "        \"\"\" Takes a user id and an item id, uses the neighborhood of the item\n",
    "        to give a weighted prediction of what the user would rate the item\"\"\"\n",
    "        weigthed_scores = list()\n",
    "        similarities = list()\n",
    "        for item, sim in self.neighborhood[predict_item].items():\n",
    "            if an_id in self.x2y[item]:\n",
    "                weigthed_scores.append(sim * self.x2y[item][an_id])\n",
    "                similarities.append(sim)\n",
    "        if not sum(similarities):\n",
    "            return -1\n",
    "        return float(f\"{sum(weigthed_scores) / sum(similarities):.2f}\")\n",
    "    \n",
    "    def recommend_item(self, an_id, reset = False):\n",
    "        \"\"\" Gets the similarities, neighborhoods and the target vector\n",
    "        Then gets predictions according to the vector. Finally recommends the top\n",
    "        predictions.\"\"\"\n",
    "        if reset:\n",
    "            self.find_all_similarities()\n",
    "            self.get_neighborhoods()\n",
    "        if not self.quiet: print(\"Recommending...\")\n",
    "        recommendations = dict()\n",
    "        for item in self.neighborhood:\n",
    "            recommendations[item] = self.getPredictionsForItems(an_id, item)\n",
    "        return dict(sorted(list(recommendations.items()), key=lambda x:x[1], reverse=True)[:self.n])\n",
    "    \n",
    "    def save_state(self, name):\n",
    "        \"\"\" To save the state to prevent recomputing each time\"\"\"\n",
    "        file_stream = open(f'data/{name}_similarities.pkl', 'wb')\n",
    "        file_stream2 = open(f'data/{name}_neighboorhood.pkl', 'wb')\n",
    "        pickle.dump(self.similarities, file_stream)\n",
    "        pickle.dump(self.neighborhood, file_stream2)\n",
    "                          \n",
    "    def load_state(self, name):\n",
    "        \"\"\" To load the state to prevent recomputing each time\"\"\"\n",
    "        file_stream = open(f'data/{name}_similarities.pkl', 'rb')\n",
    "        file_stream2 = open(f'data/{name}_neighboorhood.pkl', 'rb')\n",
    "        self.similarities = pickle.load(file_stream)\n",
    "        self.neighborhood = pickle.load(file_stream2)\n",
    "    \n",
    "    def divide_test_set(self, x2y_new, ratio = 0.8):\n",
    "        train_set = dict()\n",
    "        test_set = dict()\n",
    "        \n",
    "        # Divide the\n",
    "        if not self.quiet: print(\"Creating profiles...\")\n",
    "        keys = self.x2y.keys()\n",
    "        for item, neigh in x2y_new.items():\n",
    "            item_list = list(neigh.items())\n",
    "            test_set[item] = dict(item_list[int(len(item_list)*ratio):])\n",
    "            train_set[item] = dict(item_list[:int(len(item_list)*ratio)])\n",
    "            if item in keys:\n",
    "                self.x2y[item] = {**self.x2y[item], **train_set[item]}\n",
    "        return train_set, test_set\n",
    "    \n",
    "    def evaluate(self, x2y_new, train_set=None, test_set=None, ratio = 0.8):\n",
    "        \"\"\" Evaluates the model success via Mean Square Value. Ratio is the test to training\n",
    "        split ratio of each item neighborhood. \"\"\"\n",
    "        if not train_set:\n",
    "            train_set, test_set = self.divide_test_set(x2y_new, ratio)\n",
    "        \n",
    "        # Enter the new profiles to the neighborhoods\n",
    "        if not self.quiet: print(\"Updating...\")\n",
    "        self.update_similarities(train_set)\n",
    "        self.get_neighborhoods()\n",
    "        \n",
    "        if not self.quiet: print(\"Evaluating...\")\n",
    "        costs = dict()\n",
    "        missed_item = 0\n",
    "        # Iterate over every item to be predicted\n",
    "        for test_item, _ in x2y_new.items():\n",
    "            costs[test_item] = 0\n",
    "            missed_person = 0\n",
    "            # Iterate over every person in the test set that haven't been added to memory\n",
    "            for test_person, rate in test_set[test_item].items():\n",
    "                predicted = self.getPredictionsForItems(test_person, test_item)\n",
    "                # Predicted will be -1 if no prediction can be made, the length is adjusted\n",
    "                if predicted is -1:\n",
    "                    missed_person -= 1\n",
    "                costs[test_item] += (rate-predicted)**2\n",
    "            # Total length also being ajusted if no prediction about the item could be made\n",
    "            if len(test_set[test_item]) + missed_person is 0:\n",
    "                costs[test_item] = 0\n",
    "                missed_item -= 1\n",
    "            else:\n",
    "                costs[test_item] = costs[test_item]/(len(test_set[test_item]) + missed_person)\n",
    "        # If all predictions fail, return 30, above the possible cost\n",
    "        if len(x2y_new)+ missed_item is 0:\n",
    "            return 30\n",
    "        else:\n",
    "            return sum(costs.values())/(len(x2y_new) + missed_item)\n",
    "    \n",
    "    def optimize(self, x2y_train, x2y_test, measures, k_range):\n",
    "        \"\"\" Optimizer takes a training, a test and a measures dictionary.\n",
    "        Also, it takes either a range or a list of k values. Then it experiments\n",
    "        with all of them to find the optimal values.\"\"\"\n",
    "        \n",
    "        # Setting the variables, division of test set to profiling and predicting\n",
    "        self.x2y = x2y_train\n",
    "        self.find_all_similarities()\n",
    "        test_training, test_testing = self.divide_test_set(x2y_test)\n",
    "        \n",
    "        # Testing of each similarity measure with default k\n",
    "        costs = dict()\n",
    "        for name, measure in measures.items():\n",
    "            self.sim_measure = measure\n",
    "            costs[name] = self.evaluate(x2y_test, train_set=test_training, test_set=test_testing)\n",
    "        optimum_mes = min(list(costs.items()), key=lambda x:x[1])\n",
    "        \n",
    "        print(f\"Optimum measure is: {optimum_mes[0]} with cost {costs[optimum_mes[0]]}\")\n",
    "        self.sim_measure = measures[optimum_mes[0]]\n",
    "        \n",
    "        # Testing of each k value with the optimum measure\n",
    "        costs = dict()\n",
    "        for temp_k in k_range:\n",
    "            self.k = temp_k\n",
    "            costs[temp_k] = self.evaluate(x2y_test, train_set=test_training, test_set=test_testing)\n",
    "            print(f\"With K: {temp_k} cost is: {costs[temp_k]}\")\n",
    "        optimum_k = min(list(costs.items()), key=lambda x:x[1])[0]\n",
    "        \n",
    "        # Getting the optimum combination and testing it\n",
    "        print(f\"Optimum K value is: {optimum_k} with cost {costs[optimum_k]}\")\n",
    "        self.k = optimum_k\n",
    "        cost = self.evaluate(x2y_test, train_set=test_training, test_set=test_testing)\n",
    "        \n",
    "        return [optimum_mes[0], optimum_k, cost]\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building object...\n",
      "Done.\n",
      "Optimum measure is: cosine with cost 3.523769810901001\n",
      "With K: 5 cost is: 7.056626445264453\n",
      "With K: 10 cost is: 6.091338586309523\n",
      "With K: 20 cost is: 5.193909600642929\n",
      "With K: 50 cost is: 4.093544473197786\n",
      "With K: 100 cost is: 3.523769810901001\n",
      "Optimum K value is: 100 with cost 3.523769810901001\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['cosine', 100, 3.523769810901001]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This code initializes and optimizes the object\n",
    "measure2function = {\"euclidean\" : euclidean_similarity, \"cosine\": cosine_similarity, \"pearson\": pearson_correlation}\n",
    "aKNN = kNN(item2user, cosine_similarity)\n",
    "aKNN.optimize(item2user, item2user_test, measure2function, [5, 10, 20, 50, 100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "run_control": {
     "marked": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making a similarity matrix...\n",
      "Setting up the 10 neighbourhoods...\n",
      "Recommending...\n",
      "Making a similarity matrix...\n",
      "Setting up the 10 neighbourhoods...\n",
      "Given: 5.0\n",
      "Predicted: 0\n",
      "Given: 5.0\n",
      "Predicted: 0\n",
      "Given: 5.0\n",
      "Predicted: 0\n",
      "Given: 5.0\n",
      "Predicted: 0\n",
      "Given: 4.0\n",
      "Predicted: 0\n",
      "Given: 5.0\n",
      "Predicted: 0\n",
      "Given: 5.0\n",
      "Predicted: 0\n",
      "Given: 5.0\n",
      "Predicted: 0\n",
      "Given: 5.0\n",
      "Predicted: 0\n",
      "Given: 5.0\n",
      "Predicted: 0\n",
      "Given: 4.0\n",
      "Predicted: 0\n",
      "Given: 5.0\n",
      "Predicted: 0\n",
      "Given: 5.0\n",
      "Predicted: 0\n",
      "Given: 4.0\n",
      "Predicted: 0\n",
      "Given: 5.0\n",
      "Predicted: 0\n",
      "Given: 4.0\n",
      "Predicted: 0\n",
      "Given: 5.0\n",
      "Predicted: 0\n",
      "Given: 5.0\n",
      "Predicted: 0\n",
      "Given: 5.0\n",
      "Predicted: 0\n",
      "Given: 5.0\n",
      "Predicted: 0\n",
      "Given: 4.0\n",
      "Predicted: 0\n",
      "Given: 5.0\n",
      "Predicted: 0\n",
      "Given: 5.0\n",
      "Predicted: 0\n",
      "Given: 5.0\n",
      "Predicted: 0\n",
      "Given: 5.0\n",
      "Predicted: 0\n",
      "Given: 5.0\n",
      "Predicted: 0\n",
      "Given: 5.0\n",
      "Predicted: 0\n",
      "Given: 5.0\n",
      "Predicted: 0\n",
      "Given: 5.0\n",
      "Predicted: 0\n",
      "Given: 3.0\n",
      "Predicted: 0\n",
      "Given: 5.0\n",
      "Predicted: 0\n",
      "Given: 5.0\n",
      "Predicted: 0\n",
      "Given: 5.0\n",
      "Predicted: 0\n",
      "Given: 5.0\n",
      "Predicted: 0\n",
      "Given: 5.0\n",
      "Predicted: 0\n",
      "Given: 4.0\n",
      "Predicted: 0\n",
      "Given: 5.0\n",
      "Predicted: 0\n",
      "Given: 5.0\n",
      "Predicted: 0\n",
      "Given: 5.0\n",
      "Predicted: 0\n",
      "Given: 5.0\n",
      "Predicted: 0\n",
      "Given: 5.0\n",
      "Predicted: 0\n",
      "Given: 5.0\n",
      "Predicted: 0\n",
      "23.333333333333332\n"
     ]
    }
   ],
   "source": [
    "bKNN = kNN(item2user, pearson_correlation, name = 'item2user')\n",
    "recs = bKNN.recommend_item('A22IK3I6U76GX0')\n",
    "# This can be all zeroes, if the user happens to be in the test set. Then change the name\n",
    "\n",
    "# for ID, rate in recs.items():\n",
    "#     print(f\"Item {ID} is estimated to be rated: {rate}\")\n",
    "#print(\"Total evaluation:\", aKNN.evaluate(user2item_test))\n",
    "bKNN.evaluate(user2item_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0029870515270812572\n",
      "0.0043760251972968945\n",
      "0.004701969652065214\n",
      "0.004958882660037224\n"
     ]
    }
   ],
   "source": [
    "def recommend(userid, sim_measure, user2item, k, n):\n",
    "    \"\"\"This function takes a user id, similarity measure, the user2product dictionary, and k as input. It calculates the\n",
    "    k most similar users. It outputs a dictionary, with as keys tuples containing the k similar users ID and their\n",
    "    similarity, and as values a dictionary containing the ratings for all of their items\"\"\"\n",
    "    reviewers = [user for user in user2item.keys() if user != userid]\n",
    "    similarities = [(other_user, calc_similarity(user2item, userid, other_user, sim_measure)) for other_user in reviewers]\n",
    "    k_similarities = sorted(similarities, key = lambda x: x[1], reverse=True)[:k]\n",
    "    output = dict()\n",
    "    for user_tup in k_similarities:\n",
    "        output[user_tup] = user2item[user_tup[0]]\n",
    "    \n",
    "    lists = rankids(output, 2.49)\n",
    "    #return(output)\n",
    "    toplist = lists[0]\n",
    "    botlist = lists[1]\n",
    "    return (toplist + botlist)[:n]\n",
    "    \n",
    "\n",
    "#Evaluation function, which compares items within test users and\n",
    "#the recommendations for that user, based on a fraction of their purchase data.\n",
    "def evaluate2(x2y,k,n,measure, testdata, ratio):\n",
    "    accuracies =[]\n",
    "    for user,group in testdata.items():\n",
    "        reckeys = recommend(user, measure, x2y, k,n)\n",
    "        overlap = 0\n",
    "        #print(overlap)\n",
    "        itemscount = 0\n",
    "        iter = 0\n",
    "        lim = math.trunc(len(group) * ratio)\n",
    "        for item,score in reckeys:\n",
    "            iter += 1\n",
    "            itemscount += 1\n",
    "            if iter == lim:\n",
    "                break\n",
    "            elif item in group.keys():\n",
    "                overlap += 2\n",
    "            else:\n",
    "                pass\n",
    "        accuracy = overlap / (len(reckeys)+ itemscount)\n",
    "        accuracies.append(accuracy)\n",
    "    eval2 = np.mean(accuracies)  \n",
    "    return eval2\n",
    "    \n",
    "    \n",
    "print(evaluate2(user2item,10,10,cosine_similarity,user2item_test, 0.3))\n",
    "print(evaluate2(user2item,10,10,cosine_similarity,user2item_test, 0.5))\n",
    "print(evaluate2(user2item,10,10,cosine_similarity,user2item_test, 0.7))\n",
    "print(evaluate2(user2item,10,10,cosine_similarity,user2item_test, 0.9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "def optimiser(k_list, n_list, sim_measures, ratio, test_data):\n",
    "    scores = []\n",
    "    for k in k_list:\n",
    "        for n in n_list:\n",
    "            for measure in sim_measures:\n",
    "                score = evaluate(x2y, k, n, measure, test_data, ratio)\n",
    "                string = ''.join('k=',k,', n=',n,', measure=',measure,', ratio=',ratio)\n",
    "                scores.append((string, score))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
