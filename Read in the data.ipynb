{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from operator import itemgetter\n",
    "from scipy.spatial import distance\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from collections import defaultdict\n",
    "from itertools import product, combinations\n",
    "from operator import itemgetter\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# These are the functions you need to read in the data. You need to have the datafiles in a data folder that is in\n",
    "# the directory you are working in\n",
    "#LINK IM USING FOR K-NN https://stackabuse.com/k-nearest-neighbors-algorithm-in-python-and-scikit-learn/\n",
    "\n",
    "\n",
    "def parse(path):\n",
    "    g = gzip.open(path, 'rb')\n",
    "    for l in g:\n",
    "        yield eval(l)\n",
    "\n",
    "def getData(name, attributes):\n",
    "    i = 0\n",
    "    dic = {}\n",
    "    path = 'data/reviews_%s_5.json.gz' % name\n",
    "    for line in parse(path):\n",
    "        filtered = {}\n",
    "        for k in line.keys():\n",
    "            if k in attributes:\n",
    "                filtered[k] = line[k]\n",
    "            else:\n",
    "                continue\n",
    "        \n",
    "        dic[i] = filtered\n",
    "        i += 1\n",
    "    return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# These are one of the three names you can choose to read the dataset from\n",
    "names = ['Digital_Music', 'Kindle_Store', 'Video_Games']\n",
    "\n",
    "# These are all the possible attributes that our datapoints can have. Lots of them are not that useful for us. So during\n",
    "# the reading of the datafile you have to specify which of these attributes you want to include. For our purposes,\n",
    "# only reviewerID, asin (product ID) and overall will be helpful. But I included them all, just in case.\n",
    "attributes = ['reviewerID', 'asin', 'reviewerName', 'helpful', 'reviewText', 'overall', 'summary', \n",
    "              'unixReviewTime', 'reviewTime']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "run_control": {
     "marked": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# You read in the data for example like this:\n",
    "data = getData('Digital_Music', ['reviewerID', 'asin', 'overall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "run_control": {
     "marked": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reviewerID': 'A3EBHHCZO6V2A4', 'asin': '5555991584', 'overall': 5.0}\n",
      "{'reviewerID': 'AZPWAXJG9OJXV', 'asin': '5555991584', 'overall': 5.0}\n",
      "{'reviewerID': 'A38IRL0X2T4DPF', 'asin': '5555991584', 'overall': 5.0}\n",
      "{'reviewerID': 'A22IK3I6U76GX0', 'asin': '5555991584', 'overall': 5.0}\n",
      "{'reviewerID': 'A1AISPOIIHTHXX', 'asin': '5555991584', 'overall': 4.0}\n"
     ]
    }
   ],
   "source": [
    "# This is what our data will look like. You have a dictionary with integers as keys [0,1,2,3,4...], and as values one \n",
    "# datapoint. Each datapoint in itself is a dictionary, with as keys the attribute, and as value the value of that attribute.\n",
    "for i in range(5):\n",
    "    print(data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "def split_data(data, test_ratio=0.1):\n",
    "    reviewers = set()\n",
    "    for i in data.items():\n",
    "        reviewers.add(i[1]['reviewerID'])\n",
    "    \n",
    "    l = len(reviewers)\n",
    "    test_reviewers = set(random.sample(reviewers, int(l*test_ratio)))\n",
    "    train_reviewers = reviewers - test_reviewers\n",
    "    \n",
    "    train_data = {k:v for k,v in data.items() if v['reviewerID'] in train_reviewers}\n",
    "    test_data = {k:v for k,v in data.items() if v['reviewerID'] in test_reviewers}\n",
    "    \n",
    "    \n",
    "    tr = len(train_data)\n",
    "    te = len(test_data)\n",
    "    print('There are %s train reviews' %tr)\n",
    "    print('There are %s test reviews' %te)\n",
    "    \n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "run_control": {
     "marked": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 57901 train reviews\n",
      "There are 6805 test reviews\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = split_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "run_control": {
     "marked": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of colums in training Dataframe :  3\n",
      "Number of rows in training Dataframe :  57901\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>AZPWAXJG9OJXV</td>\n",
       "      <td>5555991584</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>A38IRL0X2T4DPF</td>\n",
       "      <td>5555991584</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>A22IK3I6U76GX0</td>\n",
       "      <td>5555991584</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>A2P49WD75WHAG5</td>\n",
       "      <td>5555991584</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>A3O90G1D7I5EGG</td>\n",
       "      <td>5555991584</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       reviewerID        asin  overall\n",
       "1   AZPWAXJG9OJXV  5555991584      5.0\n",
       "2  A38IRL0X2T4DPF  5555991584      5.0\n",
       "3  A22IK3I6U76GX0  5555991584      5.0\n",
       "5  A2P49WD75WHAG5  5555991584      5.0\n",
       "6  A3O90G1D7I5EGG  5555991584      3.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_train = pd.DataFrame.from_dict(train_data, orient='index')\n",
    "ds_test = pd.DataFrame.from_dict(test_data, orient='index')\n",
    "\n",
    "print('Number of colums in training Dataframe : ', len(ds_train.columns))\n",
    "print('Number of rows in training Dataframe : ', len(ds_train.index))\n",
    "ds_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['AZPWAXJG9OJXV' '5555991584' 5.0]\n",
      " ['A38IRL0X2T4DPF' '5555991584' 5.0]\n",
      " ['A22IK3I6U76GX0' '5555991584' 5.0]\n",
      " ['A2P49WD75WHAG5' '5555991584' 5.0]\n",
      " ['A3O90G1D7I5EGG' '5555991584' 3.0]\n",
      " ['A3EJYJC25OJVKK' '5555991584' 5.0]]\n",
      "[['A3EBHHCZO6V2A4' '5555991584' 5.0]\n",
      " ['A1AISPOIIHTHXX' '5555991584' 4.0]\n",
      " ['A19YHEBK099R7U' '5555991584' 4.0]\n",
      " ['APN6DO7VHDLTN' 'B00000016T' 5.0]\n",
      " ['A2TX79GR278JMA' 'B00000016T' 5.0]\n",
      " ['A5KJVGJ43ZDC9' 'B00000016T' 5.0]]\n"
     ]
    }
   ],
   "source": [
    "X_train = ds_train.iloc[:,:].values\n",
    "X_test = ds_test.iloc[:,:].values\n",
    "\n",
    "print(X_train[:6])\n",
    "print(X_test[:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "def cosine_similarity(p,q):\n",
    "    d = sum(pi * qi for pi,qi in zip(p, q))\n",
    "    mag_p = math.sqrt(sum([pi**2 for pi in p]))\n",
    "    mag_q = math.sqrt(sum([qi**2 for qi in q]))\n",
    "    sim = d / ( mag_p * mag_q)\n",
    "    return sim\n",
    "\n",
    "def euclidean_similarity(p, q):\n",
    "    dist = math.sqrt(sum((pi-qi)**2 for pi,qi in zip(p, q)))\n",
    "    sim = 1 / (1+dist)\n",
    "    return sim    \n",
    "\n",
    "def pearson_correlation(p,q):\n",
    "    # this code does not scale well to large datasets. In the following, we rely on \n",
    "    # scipy.spatial.distance.correlation() to compute long vectors\n",
    "    if len(p) > 99:\n",
    "        return 1 - distance.correlation(p,q)        \n",
    "    \n",
    "    p_mean = sum(p) / len(p)\n",
    "    p_deviations = [(pi-p_mean) for pi in p]\n",
    "    \n",
    "    q_mean = sum(q) / len(q)\n",
    "    q_deviations = [(qi-q_mean) for qi in q]\n",
    "    \n",
    "    cov = sum(pd * qd for pd,qd in zip(p_deviations, q_deviations))\n",
    "        \n",
    "    sds_product = math.sqrt(sum((pd)**2 for pd in p_deviations) * sum((qd)**2 for qd in q_deviations))\n",
    "    \n",
    "    if sds_product != 0:\n",
    "        r = cov / sds_product\n",
    "    else:\n",
    "        r = 0\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "def calc_similarity(user2product, target, other_user, sim_measure, threshold=0):\n",
    "    # found some explanation here https://towardsdatascience.com/introduction-to-recommender-systems-6c66cf15ada\n",
    "    \n",
    "    shared = list(set(user2product[target].keys()).intersection(set(user2product[other_user].keys())))\n",
    "    \n",
    "    if len(shared) <= threshold:\n",
    "        return 0\n",
    "    \n",
    "    target_ratings = [v for k,v in user2product[target].items() if k in shared]# for i in [target, other_user]]\n",
    "    other_user_ratings = [v for k,v in user2product[other_user].items() if k in shared]\n",
    "    \n",
    "    weight = len(shared)/len(user2product[target])\n",
    "    similarity = weight*sim_measure(target_ratings, other_user_ratings)\n",
    "    \n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "run_control": {
     "marked": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "user2item = defaultdict(dict)\n",
    "for reviewerID, asin, overall in X_train:\n",
    "    user2item[reviewerID][asin] = overall\n",
    "\n",
    "user2item_test = defaultdict(dict)\n",
    "for reviewerID, asin, overall in X_test:\n",
    "    user2item_test[reviewerID][asin] = overall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "run_control": {
     "marked": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no. of users: 4987\n",
      "no of reviews in user 1: 16\n"
     ]
    }
   ],
   "source": [
    "print(\"no. of users:\", len(user2item))\n",
    "print(\"no of reviews in user 1:\", len(list(user2item.items())[0][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no. of items: 3567\n",
      "no. of items: 2524\n"
     ]
    }
   ],
   "source": [
    "item2user = defaultdict(dict)\n",
    "for reviewerID, asin, overall in X_train:\n",
    "    item2user[asin][reviewerID] = overall\n",
    "    \n",
    "print(\"no. of items:\", len(item2user))\n",
    "\n",
    "item2user_test = defaultdict(dict)\n",
    "for reviewerID, asin, overall in X_test:\n",
    "    item2user_test[asin][reviewerID] = overall\n",
    "\n",
    "print(\"no. of items:\", len(item2user_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# Calculate the similarities (NB: this can take a minute)\n",
    "measure2function = {\"euclidean\" : euclidean_similarity, \"cosine\": cosine_similarity, \"pearson\": pearson_correlation}\n",
    "\n",
    "similarities = {\"euclidean\" : dict(), \"cosine\": dict(), \"pearson\": dict()}\n",
    "for measure, function in measure2function.items():\n",
    "    similarities[measure] = dict([(a,dict()) for a in list(user2item.keys())[:40]])\n",
    "    for id1, id2, in combinations(list(user2item.keys())[:40], 2):\n",
    "        similarities[measure][id1][id2] = calc_similarity(user2item, id1, id2, function)\n",
    "        \n",
    "#print(similarities[\"cosine\"].items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "def rankids(useritemdict, thresh):\n",
    "    rankedt = []\n",
    "    rankedb = []\n",
    "    scoredict = {}\n",
    "    \n",
    "    for k, v in useritemdict.items():\n",
    "        itemdict = v\n",
    "        for itemID, stars in itemdict.items():\n",
    "            p = (stars, k[1])\n",
    "            if itemID in scoredict:\n",
    "                scoredict[itemID].append(p)\n",
    "            else:\n",
    "                scoredict[itemID] = [p]\n",
    "    for ID, IDscortup in scoredict.items():\n",
    "        starlist = []\n",
    "        for scortup in IDscortup:\n",
    "            starlist.append(scortup[0])\n",
    "        a = np.mean(starlist)\n",
    "        q = []\n",
    "        for r in IDscortup:\n",
    "            q.append(r[0] * r[1])\n",
    "        IDandweighted = (ID, np.mean(q))\n",
    "        if a > thresh:\n",
    "            rankedt.append(IDandweighted)\n",
    "        else:\n",
    "            rankedb.append(IDandweighted)            \n",
    "    \n",
    "    rankedt.sort(key=itemgetter(1))\n",
    "    rankedb.sort(key=itemgetter(1))\n",
    "    rankedt.reverse()\n",
    "    rankedb.reverse()\n",
    "    return(rankedt, rankedb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "def recommend(userid, sim_measure, user2item, k):\n",
    "    \"\"\"This function takes a user id, similarity measure, the user2product dictionary, and k as input. It calculates the\n",
    "    k most similar users. It outputs a dictionary, with as keys tuples containing the k similar users ID and their\n",
    "    similarity, and as values a dictionary containing the ratings for all of their items\"\"\"\n",
    "    reviewers = [user for user in user2item.keys() if user != userid]\n",
    "    similarities = [(other_user, calc_similarity(user2item, userid, other_user, sim_measure)) for other_user in reviewers]\n",
    "    k_similarities = sorted(similarities, key = lambda x: x[1], reverse=True)[:k]\n",
    "    output = dict()\n",
    "    for user_tup in k_similarities:\n",
    "        output[user_tup] = user2item[user_tup[0]]\n",
    "    \n",
    "    lists = rankids(output, 2.49)\n",
    "    #return(output)\n",
    "    toplist = lists[0]\n",
    "    botlist = lists[1]\n",
    "    print(toplist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "#recommend('AZPWAXJG9OJXV', cosine_similarity, user2item, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "class kNN(object):\n",
    "    \"\"\" k-Nearest Neighbour \"\"\"\n",
    "    \n",
    "    def __init__(self, x2y: dict, sim_measure, name=None, k = 10):\n",
    "        self.x2y = x2y                     # For example, user2item matrix\n",
    "        self.k = k\n",
    "        self.sim_measure = sim_measure     # Like cosine similarity\n",
    "        if name:\n",
    "            self.similarities=pickle.load(open(f\"data/{name}_similarities.pkl\", 'rb'))\n",
    "            self.neighborhood=pickle.load(open(f\"data/{name}_neighboorhood.pkl\", 'rb'))\n",
    "        else:\n",
    "            self.similarities = None   # If the similarities are already stored\n",
    "            self.neighborhood = None  # Likewise\n",
    "        self.quite = False\n",
    "        self.target_vector = None\n",
    "    \n",
    "    def find_id_similarities(self, an_id):\n",
    "        \"\"\" For user-to-user part\"\"\"\n",
    "        sims = dict()\n",
    "        for id1, id2 in product([target], list(x2y.keys())[:40]):\n",
    "            sims[id2] = calc_similarity(x2y, id1, id2, self.sim_measure)\n",
    "        return sorted(list(sims.items()), key=lambda x:x[1], reverse = True)[:k]\n",
    "    \n",
    "    def find_all_similarities(self, reset):\n",
    "        \"\"\" For item-to-item part\"\"\"\n",
    "        if not self.quite: print(\"Making a similarity matrix...\")\n",
    "        if not reset: return\n",
    "        sims = defaultdict(dict)\n",
    "        for id1, id2 in combinations(self.x2y.keys(), 2):\n",
    "            sims[id1][id2] = calc_similarity(self.x2y, id1, id2, self.sim_measure)\n",
    "        self.similarities = sims\n",
    "    \n",
    "    def get_neighborhoods(self, reset):\n",
    "        \"\"\" For every x, gets the other y similarities and makes it a dictionary\"\"\"\n",
    "        if not self.quite: print(f\"Setting up the {self.k} neighbourhoods...\")\n",
    "        if not reset: return\n",
    "        self.neighborhood = dict()\n",
    "        for x in self.similarities.keys():\n",
    "            self.neighborhood[x] = dict(sorted(self.similarities[x].items(),\n",
    "                                               key = itemgetter(1),\n",
    "                                               reverse = True)[:self.k])\n",
    "    \n",
    "    def built_target(self, an_id):\n",
    "        \"\"\" Makes a target vector from everything the user didn't rate\"\"\"\n",
    "        self.target_vector = list()\n",
    "        for ID, group in self.neighborhood.items():\n",
    "            if an_id not in group.keys():\n",
    "                self.target_vector.append(ID)\n",
    "    \n",
    "    def getPredictionsForItems(self, an_id, x):\n",
    "        \"\"\" Creates rating predictions depending on the neighborhood of x\"\"\"\n",
    "        weigthed_scores = list()\n",
    "        similarities = list()\n",
    "        self.target_vector = list()\n",
    "        for item, sim in self.neighborhood[x].items():\n",
    "            if an_id in self.x2y[item]:\n",
    "                weigthed_scores.append(sim * self.x2y[item][an_id])\n",
    "                self.target_vector.append(item)\n",
    "                similarities.append(sim)\n",
    "        if not sum(similarities):\n",
    "            return 0\n",
    "        return float(f\"{sum(weigthed_scores) / sum(similarities):.2f}\")\n",
    "    \n",
    "    def recommend_item(self, an_id, n = 10, reset = False):\n",
    "        \"\"\" Gets the similarities, neighborhoods and the target vector\n",
    "        Then gets predictions according to the vector. Finally recommends the top\n",
    "        predictions.\"\"\"\n",
    "        self.find_all_similarities(reset)\n",
    "        self.get_neighborhoods(reset)\n",
    "        self.built_target(an_id)\n",
    "        if not self.quite: print(\"Recommending...\")\n",
    "        recommendations = dict()\n",
    "        for x in self.target_vector:\n",
    "            recommendations[x] = self.getPredictionsForItems(an_id, x)\n",
    "        return dict(sorted(list(recommendations.items()), key=lambda x:x[1], reverse=True)[:n])\n",
    "    \n",
    "    def save_state(self, name):\n",
    "        \"\"\" To save the state to prevent recomputing each time\"\"\"\n",
    "        file_stream = open(f'data/{name}_similarities.pkl', 'wb')\n",
    "        file_stream2 = open(f'data/{name}_neighboorhood.pkl', 'wb')\n",
    "        pickle.dump(self.similarities, file_stream)\n",
    "        pickle.dump(self.neighborhood, file_stream2)\n",
    "                          \n",
    "    def load_state(self, name):\n",
    "        \"\"\" To save the state to prevent recomputing each time\"\"\"\n",
    "        file_stream = open(f'data/{name}_similarities.pkl', 'rb')\n",
    "        file_stream2 = open(f'data/{name}_neighboorhood.pkl', 'rb')\n",
    "        self.similarities = pickle.load(file_stream)\n",
    "        self.neighborhood = pickle.load(file_stream2)\n",
    "    \n",
    "    def evaluate(self, x2y, ratio = 0.8):\n",
    "        costs = 0\n",
    "        train_lengths = list()\n",
    "        x2y_list = [(key, list(group.items())) for key, group in x2y.items()]\n",
    "        for key, group in x2y_list:\n",
    "            train_lengths.append(int(len(x2y[key])*ratio))\n",
    "            self.x2y[key] = dict(group[:train_lengths[-1]])\n",
    "        \n",
    "        self.find_all_similarities(True)\n",
    "        self.get_neighborhoods(True)\n",
    "        self.quite = True\n",
    "        missed = 0\n",
    "        for m, (outer_key, group) in enumerate(x2y_list):\n",
    "            self.target_vector = dict(group[train_lengths[m]:])\n",
    "            recommendations = self.recommend_item(outer_key, n=-1)\n",
    "            cost = self.our_MSE(recommendations)\n",
    "            if cost is not -1:\n",
    "                costs += cost\n",
    "            else:\n",
    "                missed -= 1\n",
    "        self.quite = False\n",
    "        return costs\n",
    "        \n",
    "    def our_MSE(self, recommendations):\n",
    "        costs = 0\n",
    "        for key, rating in self.target_vector:\n",
    "            costs += (rating-recommendations[key])**2\n",
    "        if len(self.target_vector) is 0:\n",
    "            return -1\n",
    "        return costs/len(self.target_vector)\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making a similarity matrix...\n",
      "Setting up the 10 neighbourhoods...\n"
     ]
    }
   ],
   "source": [
    "# This code initializes and populates the object\n",
    "aKNN = kNN(item2user, pearson_correlation)\n",
    "aKNN.find_all_similarities(reset=True)\n",
    "aKNN.get_neighborhoods(reset=True)\n",
    "aKNN.save_state('item2user')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making a similarity matrix...\n",
      "Setting up the 10 neighbourhoods...\n",
      "Recommending...\n",
      "Item B000001EP1 is estimated to be rated: 5.0\n",
      "Item B000002VLB is estimated to be rated: 4.63\n",
      "Item B0000009UW is estimated to be rated: 4.0\n",
      "Item B000000OU9 is estimated to be rated: 4.0\n",
      "Item B000002INP is estimated to be rated: 4.0\n",
      "Item B000002IT2 is estimated to be rated: 4.0\n",
      "Item B000002IXU is estimated to be rated: 4.0\n",
      "Item B000002J88 is estimated to be rated: 4.0\n",
      "Item B000002KME is estimated to be rated: 4.0\n",
      "Item B000002LGQ is estimated to be rated: 4.0\n"
     ]
    }
   ],
   "source": [
    "bKNN = kNN(item2user, pearson_correlation, name = 'item2user')\n",
    "recs = bKNN.recommend_item('A22IK3I6U76GX0')\n",
    "# This can be all zeroes, if the user happens to be in the test set. Then change the name\n",
    "\n",
    "for ID, rate in recs.items():\n",
    "    print(f\"Item {ID} is estimated to be rated: {rate}\")\n",
    "#print(\"Total evaluation:\", aKNN.evaluate(user2item_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'user2product' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-edd39b28c494>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m#amount_of_ratings_per_user[key] = len(user2product.get(key))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mtarget_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup\u001b[0m \u001b[0;32min\u001b[0m \u001b[0muser2product\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mtarget_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser2product\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'user2product' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "#user to user evaluation\n",
    "#print(user2product)\n",
    "#type(user2product)\n",
    "\n",
    "#len of ratings of each user\n",
    "#amount_of_ratings_per_user = dict()\n",
    "#for key in user2product:\n",
    "    #amount_of_ratings_per_user[key] = len(user2product.get(key))\n",
    "target_values = dict()\n",
    "for key, group in user2product.items():\n",
    "    a = list(group.items())\n",
    "    target_values[key] = dict(a[:int((len(user2product.get(key)))/5)])\n",
    "    tomakeprofile[key] = dict(a[int((len(user2product.get(key)))/5):])\n",
    "\n",
    "print(target_values['A3EBHHCZO6V2A4']['5555991584'])\n",
    "\n",
    "costs = defaultdict(dict)\n",
    "l = 0\n",
    "r = 0\n",
    "for outer_key, group in target_values.items():\n",
    "    recons = recommendation(outer_key)\n",
    "    for inner_key, rating in group.items():\n",
    "        costs[outer_key][inner_key] = mean_absolute_error(recons[inner_key], rating)\n",
    "        r =+ mean_absolute_error(recons[inner_key], rating)\n",
    "        l =+ 1\n",
    "\n",
    "average_error = r/l\n",
    "#print(recommend(tomakeprofile).items())\n",
    "\n",
    "predicted = dict() \n",
    "#for key in tomakeprofile:\n",
    "    #predicted[key] = recommendba(key,cosine_similarity,tomakeprofile,10)\n",
    "\n",
    "#print(mean_absolute_error(true_ratings, predicted))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
